{"basic_builder": {"appname": "TA-axonius", "friendly_name": "Axonius Add-On for Splunk", "version": "1.3.10", "author": "Axonius", "description": "The Axonius Splunk Technology Add-on is designed to help your organization offload data from Axonius into Splunk. The TA can be configured to use the existing saved queries already available in your environment.", "theme": "#ffffff", "large_icon": "iVBORw0KGgoAAAANSUhEUgAAAEgAAABICAYAAABV7bNHAAAAAXNSR0IArs4c6QAABw5JREFUeF7tnHmMXXMUxz93pgslGtQSpVNVtNRWW9BQf1DEFqWxU8u0hKYoIRKaShBL/EGrHVFLUUJsSal9SS1RSxFFlalp7UuE6rSddq58x73j9Xnz7u/87u29b8JJ+kfnnfX7fvfc3zm/83sB61IvYB6wb9nfdwUWlv2tu/+3P7CsLIiPgX2A1fHfgxKGc4C7q0T9G7Bpd0cl8v8XYLMqsVwB3KzPY4BGA485BP8tIOS7M7UA2zkEcCYwSwDVAWsdBGKWE4AnDPy1xHoo8LLBoR4CaCww0yD0NTDQwF9LrAuAPQwOTRZAoUGgUu7yEC9MxBzr/wAlfFe+AG0ArCpsHfgZ7gf8ZBUVQGcA9xsFhwKfGWWKZt8feNvoxAQBpH/tRsELgTuNMkWzW19G8rc+3gcdBjxviOBTYBcDfy2wvg/sZXDkSGBu6U76ROBRg4LNgV8N/EWybgz8YXBAaecB8ZcCFMtPBMYA+2mJVVEqcARSd6Ck0kIxLALuBW4oDagSQJUC1m5bW+/bAX0bMS2PNo1yoBZJteOXZTWkfL4YuM9lD+gKUGnw5wF3laGh1aRabg7wOdBcWhHniNwIYDgwDBgFDCizPQm41eKPD0DSf6mjIQEnXn1b64POAm4BtMdJouuAa5KYyj/3BUh61DPp6WhwfbRKXPJK7F4boF6XmdIANBm41mBRgPY28FdjXWnUpd6OejxmSgPQjlHmtxjVo5A2ofcAtCIstBPwhUUg5k0DkHRYq2PtRTbxcbRE5htgG6MO7zi9BSMHrQCtMeStrjBQkWzNJ95xegsWCJAeLz1mFvKO01sw8k6nApYedSvQxxJZBd4/jTpSPdZpAZoKqLJ3pauAG12Zu+A7G7jHoONh4BQD/zqsaQGyJuos7OVqMwuHTwUedPiGtgX0BsqCtga+c1A0HpjhwNclSxYASbk6jNVOXrf0aXcmBKZCtFq75QCPDuK/TGYFUKxYR9RaTdsDOsZVj+n7NN+gg6xOSJ8GdgN+jFrI1tbqel9BDnFUZglD6hg36ECCteqLC1jRPOrXjg2mLVvsrTgjwaxXkMmtcMLOY1i18pGqQn02GhbctvATk+IMmQsDKGwcOJsgPNkpljAcEzS1WNrBTmpdmAoByGnllHsftg8OmpaqO5gr5Q5QR84Z32AZlogAqWsLZjRba7DUYOYPUOPgwwnanvPyvK29fzBzqUZwcqP8AbqgYQXtbOgVYY+eU4Kpiy1NOi8zpUL5AzSuwdoi+cffMJwfNLXoOCo36l4ABeE7wfQWnbHnRkUA9FXJhtAWaH0wPpi2JFVtZTNY+WTVqsPEH1404CDaAk3S2insvUXQtOhnu6C/RO4rSK6GPnmovu73YFpzX/9Q/SSLAWjikJG0tr5icjnou1Uw/SMVo7lSIQB1rKLGhssIOk5Fk6lnOCK4o+WNZMbsOQoD6G+Qhg4nWPFel2HV0UrYd2ARKyf2qVCAYifCxkEDCNovoZ5RrGlfDnVz6BdMD65v/iH7NWHTmCVAGpFRV1E9nXg4/UXgaJtLZu4ro6EE2dQm9AXgWLOWLgSyAki11eFVnDoNeCgrpyM9SUOZGo4/N63NLABKAif2MUuQksCJbc4FNGvoTWkB0iGgDvJcKa29zrTlajCaAum83mSQ62BN67AutRxvMKoJtLQ5SRNiGspyJeWkao9/VT1pAbIMMckRDR5oSj8N6fjaoiOVzbQAWVsXWUx3dKvhBStAWUyZWafLUqWSvFeQJmCHpHm+gHeBvY06vOP0FgT2BD4wOqrBdOu9kHIT1huSkpevHxp9Tf0WmwWcbjCquyCaXc6CdLRs6SzqWoGuF5gpzQqy5J8swYmD1IrY3RCxV6xeQtGwwDEJzul1/HpUF3lv1BJs6K6IBheUk5JGjJ8y7tm8HjHlkGcBXZ8qJS3hKb6jtoZV4MKq2SH9OICuTBxXJqCNqgpZ5zyYtII016x7YTsAJ1Woa1SHHeHidUE8iu8lQNfBS0m3ejRN8hagbUOXVA7Q4OjXFw52COg1YKQDXy2wvAoc4uCIilu9eDqH3UsBcq3KYzsaxfU4Y3dwM3sWpQbt4l3pcUC/RtFZrDYB57tKR3noKAN/LbA+Y2x9qCiepBWkGzvWt4wGN2fXQtQGH6zjw1LdSwDdBFxuMCRWtVWXGGWKZleJo8vIFpohgCwbvlh50tvP4kRevNonVX1jVXLkvwSQ4jcvBgFkbXqVJve8vv2s7FgBWi2AtE1XC8GV5kdXxl35a4lPezeXPV7s8+g4l1iQ1U3DwueXPVHXRthy87AuBkg/mKSr3Em0PqryJJtZf/4moGsKSaSfsVhQ+jZKAkl35RuTtHaTz1XZVzt9VbHbMTNQ6XWtnKSVEv9KnG4MX+1xkbbWsdJr/8myYluHm9oAd6acvwAwy08tydtJ2AAAAABJRU5ErkJggg==", "small_icon": "iVBORw0KGgoAAAANSUhEUgAAACQAAAAkCAYAAADhAJiYAAAAAXNSR0IArs4c6QAAA6hJREFUWEe9mGuITVEUx39nvB8jbz6oGU3yTB7RyCt5hJA8EkkaMhPSeIfyGslbUma8klDCF3yQR8kjPlBqJkRipnzwGI8UGoyjP/vW2LPPOXfOda263dvda6/1O2vvtfdax+OPzAJKgEzgHrAMuG3G0vU1GNgJDAI+A4uA4x4wHThjea0COgGVaaJpCbwCGln2ZwroMdDN4XgNsC1NQCtMdGzz5QLyA5xuBjakCWgLsM5lW0DFQIFj8BQwO01Asq19a8sRAbUCbgE9rdEvgNb6+z+Gqgd8BJpbdrV1hggoIZOAvoD+m2YALwMT/yFUfeACMA54CJwzzvX7rH7XBLIDsRLYYWCuA6XmyY6aDEkmcB2BeUAboD8wEGgMrAeKgvZQmOHnQGdL4QcwErgZQTQMuAY0sPQqgOyguWER0pztwCrHZMEMjwC6Aox26Cjqq+MCbQxI/Z+ANmeYKJIunU2A7DolKkJzdJw7Zj4DukQAaaP2cOjkA4fiAmneI6C7ZWAsoAwMkzEOnadA17BJURHS3KYmxAuAO8Be4GoyKQaMApYCQ4HDgJbrU6pAgfP9gpz2+NVrwNdJXwVeCZktirxdpbq9Y0kyEXIaNjBl4Le3FB7StHqAt/fl1zhEKQBl78P3l7hTJWOrV/LCeXlGQaYAlFWNT0ZA8pZ5B8t7Rzl3jccHys/SkugaqC0e972SigH/F6ggaw/+7wyqLRneZq+4PFYtFT9ChX1a8vXDfSDnLyKPB7xrluudffTtv0ZIzvy8rpk0rFqLz5TfaZ/hn6eyWVFcGNlMJkLanKqtdfK+BxY7moKgYKjG2m1u9/OmbH2SysGoy/EN0NoyMiMJKMEIoqbogdoBupwD8iF8oXURql+z5S1gH4i2TjmQ5Zirs2t/XKCg8qMaUDkaJmkpP9RZqoey5RIwPgLoIjDBobMLUHkca8nUXXZw7IMRpsYOY+oH3HB0F68B1dp1AlL2qYiaD8iAijSdzGqtD4RtSsuL6umFptUSxFSgrbGtCqFWk5pI+yaANrCUddApxZVZx4C8iKWp67BszjWdi1ofZZ4+ali/C0gVnAovO7WlJMCgVruuIAl9+VSkbX/K3EEaPBHQMqcjOgkobQHV67acDnvZENodxA2PmSfbahZriYDuArmOseXAnhQdB00vNLW5PV4mIHUQOldqivaPXljFKkOTeAglkY6UFpbuxESWCUr1iyJ1EtD7m9BLMAmnUSq9TH8/2bw0Uzd78Re6a8rvpm5PzwAAAABJRU5ErkJggg==", "visible": true, "tab_version": "4.4.1", "tab_build_no": "0", "build_no": 9}, "data_input_builder": {"datainputs": [{"index": "default", "sourcetype": "axonius_saved_query", "interval": "60", "use_external_validation": true, "streaming_mode_xml": true, "name": "axonius_saved_query", "title": "Axonius Saved Query", "description": "", "type": "customized", "parameters": [{"name": "api_host", "label": "Axonius Host", "help_string": "The URL of the Axonius web host", "required": true, "format_type": "text", "default_value": "", "placeholder": "https://axonius.example.com", "type": "text", "value": "https://10.204.213.73"}, {"name": "entity_type", "label": "Entity Type", "help_string": "The entity type of the saved query", "required": true, "possible_values": [{"value": "devices", "label": "Devices"}, {"value": "users", "label": "Users"}, {"label": "Vulnerabilities", "value": "vulnerabilities"}, {"label": "Vulnerabilities Repository", "value": "vulnerabilities_repository"}, {"label": "Software", "value": "software"}], "format_type": "dropdownlist", "default_value": "devices", "placeholder": "", "type": "dropdownlist", "value": "users"}, {"name": "saved_query", "label": "Saved Query", "help_string": "The name of the saved query", "required": true, "format_type": "text", "default_value": "", "placeholder": "", "type": "text", "value": "All Users"}, {"name": "page_size", "label": "Page Size", "help_string": "The number of asset entities to fetch during each API call, higher is quicker while lower takes less memory.\nThe maximum value is 2,000", "required": true, "format_type": "text", "default_value": "1000", "placeholder": "", "type": "text", "value": "1000"}, {"name": "standoff_ms", "label": "API Standoff (milliseconds)", "help_string": "The number of milliseconds to wait between successive API calls", "required": true, "format_type": "text", "default_value": "0", "placeholder": "", "type": "text", "value": "0"}, {"name": "shorten_field_names", "label": "Shorten Field Names", "help_string": "Shortens the long dotted notation field names by removing the prefixes \"specific_data.data.\" and \"adapters_data.\".", "required": false, "format_type": "checkbox", "default_value": false, "type": "checkbox", "value": false}, {"name": "dynamic_field_mapping", "label": "Dynamic Field Mapping", "help_string": "Rename fields using a JSON-formatted string, renaming occurs prior to data ingest", "required": false, "format_type": "text", "default_value": "{\"hostname\": \"host\", \"network_interfaces.ips\": \"ip_address\"}", "placeholder": "", "type": "text", "value": "{\"hostname\": \"host\", \"network_interfaces.ips\": \"ip_address\"}"}, {"name": "cron_schedule", "label": "Cron Schedule", "help_string": "Use this parameter when you want to use a cron schedule to schedule the data ingestion.", "required": false, "format_type": "text", "default_value": "", "placeholder": "Cron schedule expression", "type": "text", "value": "22 17 * * *"}, {"name": "incremental_data_ingest", "label": "Incremental Ingest", "help_string": "Include only the entities that have a fetch timer newer than last collection", "required": false, "format_type": "checkbox", "default_value": true, "type": "checkbox", "value": false}, {"name": "incremental_ingest_time_field", "label": "Incremental Ingest Time Field", "help_string": "Time field to use for comparison for incremental ingest. For Vulnerabilities use specific_data.data.first_seen", "required": true, "format_type": "text", "default_value": "specific_data.data.fetch_time", "placeholder": "specific_data.data.fetch_time", "type": "text", "value": "specific_data.data.fetch_time"}, {"name": "enable_include_details", "label": "Enable \"Include Details\"", "help_string": "Enable extra information to be returned in the result set that marries fields to their source adapter.", "required": false, "format_type": "checkbox", "default_value": true, "type": "checkbox", "value": true}, {"name": "ssl_certificate_path", "label": "CA Bundle Path", "help_string": "The filesystem path to the CA bundle used for SSL certificate validation", "required": false, "format_type": "text", "default_value": "", "placeholder": "Path to CA bundle (Examples: C:/Certs/ca_bundle.pem or /home/splunk/ca_bundle.pem)", "type": "text", "value": ""}, {"name": "skip_lifecycle_check", "label": "Skip Lifecycle Check", "help_string": "This option will skip the lifecycle check. This should remained unchecked unless otherwise advised to turn on.", "required": false, "format_type": "checkbox", "default_value": false, "type": "checkbox", "value": true}], "data_inputs_options": [{"type": "customized_var", "name": "api_host", "title": "Axonius Host", "description": "The URL of the Axonius web host", "required_on_edit": false, "required_on_create": true, "format_type": "text", "default_value": "", "placeholder": "https://axonius.example.com"}, {"type": "customized_var", "name": "entity_type", "title": "Entity Type", "description": "The entity type of the saved query", "required_on_edit": false, "required_on_create": true, "possible_values": [{"value": "devices", "label": "Devices"}, {"value": "users", "label": "Users"}, {"label": "Vulnerabilities", "value": "vulnerabilities"}, {"label": "Vulnerabilities Repository", "value": "vulnerabilities_repository"}, {"label": "Software", "value": "software"}], "format_type": "dropdownlist", "default_value": "devices", "placeholder": ""}, {"type": "customized_var", "name": "saved_query", "title": "Saved Query", "description": "The name of the saved query", "required_on_edit": false, "required_on_create": true, "format_type": "text", "default_value": "", "placeholder": ""}, {"type": "customized_var", "name": "page_size", "title": "Page Size", "description": "The number of asset entities to fetch during each API call, higher is quicker while lower takes less memory.\nThe maximum value is 2,000", "required_on_edit": false, "required_on_create": true, "format_type": "text", "default_value": "1000", "placeholder": ""}, {"type": "customized_var", "name": "standoff_ms", "title": "API Standoff (milliseconds)", "description": "The number of milliseconds to wait between successive API calls", "required_on_edit": false, "required_on_create": true, "format_type": "text", "default_value": "0", "placeholder": ""}, {"type": "customized_var", "name": "shorten_field_names", "title": "Shorten Field Names", "description": "Shortens the long dotted notation field names by removing the prefixes \"specific_data.data.\" and \"adapters_data.\".", "required_on_edit": false, "required_on_create": false, "format_type": "checkbox", "default_value": false}, {"type": "customized_var", "name": "dynamic_field_mapping", "title": "Dynamic Field Mapping", "description": "Rename fields using a JSON-formatted string, renaming occurs prior to data ingest", "required_on_edit": false, "required_on_create": false, "format_type": "text", "default_value": "{\"hostname\": \"host\", \"network_interfaces.ips\": \"ip_address\"}", "placeholder": ""}, {"type": "customized_var", "name": "cron_schedule", "title": "Cron Schedule", "description": "Use this parameter when you want to use a cron schedule to schedule the data ingestion.", "required_on_edit": false, "required_on_create": false, "format_type": "text", "default_value": "", "placeholder": "Cron schedule expression"}, {"type": "customized_var", "name": "incremental_data_ingest", "title": "Incremental Ingest", "description": "Include only the entities that have a fetch timer newer than last collection", "required_on_edit": false, "required_on_create": false, "format_type": "checkbox", "default_value": true}, {"type": "customized_var", "name": "incremental_ingest_time_field", "title": "Incremental Ingest Time Field", "description": "Time field to use for comparison for incremental ingest. For Vulnerabilities use specific_data.data.first_seen", "required_on_edit": false, "required_on_create": true, "format_type": "text", "default_value": "specific_data.data.fetch_time", "placeholder": "specific_data.data.fetch_time"}, {"type": "customized_var", "name": "enable_include_details", "title": "Enable \"Include Details\"", "description": "Enable extra information to be returned in the result set that marries fields to their source adapter.", "required_on_edit": false, "required_on_create": false, "format_type": "checkbox", "default_value": true}, {"type": "customized_var", "name": "ssl_certificate_path", "title": "CA Bundle Path", "description": "The filesystem path to the CA bundle used for SSL certificate validation", "required_on_edit": false, "required_on_create": false, "format_type": "text", "default_value": "", "placeholder": "Path to CA bundle (Examples: C:/Certs/ca_bundle.pem or /home/splunk/ca_bundle.pem)"}, {"type": "customized_var", "name": "skip_lifecycle_check", "title": "Skip Lifecycle Check", "description": "This option will skip the lifecycle check. This should remained unchecked unless otherwise advised to turn on.", "required_on_edit": false, "required_on_create": false, "format_type": "checkbox", "default_value": false}], "code": "# encoding = utf-8\n\nfrom croniter import croniter\nimport datetime\nimport json\nimport re\nimport requests\nimport time\nfrom tzlocal import get_localzone\n\nfrom urllib.parse import urlparse\n\nCF_ACCESS_TOKEN = None\n\nclass Config:\n    supported_minimum_version: str = \"4.4.0\"\n    retry_standoff: list = [0, 5, 10, 15, 30, 60]\n    request_timeout: int = 900\n\n\nclass API:\n    def __init__(self, url, api_key, api_secret, verify=True, timeout=900, helper=None):\n        self._helper = helper\n        self._url = url\n        self._api_key = api_key\n        self._api_secret = api_secret\n        self._verify = verify\n        self._timeout = timeout\n\n    def _rest_base(self, method, api_endpoint, data=None, params=None, headers={}):\n        requests_method = getattr(requests, method)\n        exception = None\n        req = None\n\n        try:\n            headers['api-key'] = self._api_key\n            headers['api-secret'] = self._api_secret\n\n            if CF_ACCESS_TOKEN:\n                headers['cf-access-token'] = CF_ACCESS_TOKEN\n\n            req = requests_method(f\"{self._url}{api_endpoint}\", timeout=self._timeout, params=params,\n                                  data=json.dumps(data), headers=headers, verify=self._verify)\n\n        except Exception as e:\n            exception = e\n\n        req_status_code = None\n\n        if req is not None:\n            req_status_code = req.status_code\n\n        req_json = {\"data\": \"\"}\n\n        if req is not None:\n            req_json = req.json()\n\n        return req_status_code, req_json, exception\n\n    def get(self, api_endpoint, data=None, params=None, headers={}):\n        if 'Content-Type' not in headers:\n            headers['Content-Type'] = 'application/json'\n\n        if self._helper:\n            self._helper.log_debug(f\"Performing GET request to {api_endpoint}.\")\n            self._helper.log_debug(f\"Params: {params}\")\n            self._helper.log_debug(f\"Data: {data}\")\n\n        return self._rest_base(\"get\", api_endpoint, data=data, params=params, headers=headers)\n\n    def post(self, api_endpoint, data=None, params=None, headers={}):\n        if 'Content-Type' not in headers:\n            headers['Content-Type'] = 'application/vnd.api+json'\n\n        if self._helper:\n            self._helper.log_debug(f\"Performing POST request to {api_endpoint}.\")\n            self._helper.log_debug(f\"Params: {params}\")\n            self._helper.log_debug(f\"Data: {data}\")\n\n        return self._rest_base(\"post\", api_endpoint, data=data, params=params, headers=headers)\n\n\nclass Metadata:\n    def __init__(self, api):\n        self._api = api\n        self._api_endpoint = \"/api/settings/metadata\"\n\n    def get_version(self):\n        status, response, exception = self._api.get(self._api_endpoint)\n\n        if status == 200 and response is not None and exception is None:\n            return response[\"Installed Version\"]\n        else:\n            raise Exception(f\"Critical Error! Status Code: '{status}' Exception: '{exception}'\")\n\n\nclass Lifecycle:\n    def __init__(self, api):\n        self._api = api\n        self._api_endpoint = \"/api/dashboard/lifecycle\"\n        self._response = None\n        self._sub_phases = {}\n        self.status = None\n\n    def update(self):\n        status, response, exception = self._api.get(self._api_endpoint)\n\n        if status == 200 and response is not None and exception is None:\n            self._response = response\n            status = self._response[\"data\"][\"attributes\"][\"status\"]\n            self.status = True if \"done\" not in status else False\n\n            for sub_phase in self._response[\"data\"][\"attributes\"][\"sub_phases\"]:\n                self._sub_phases[sub_phase[\"name\"].lower()] = True if sub_phase[\"status\"] == 1 else False\n        else:\n            raise Exception(f\"Critical Error! Status Code: '{status}' Exception: '{exception}'\")\n\n    def discovery_is_running(self):\n        if not bool(self._sub_phases):\n            self.update()\n\n        return self.status\n\n    def correlation_is_complete(self):\n        if not bool(self._sub_phases):\n            self.update()\n\n        return self._sub_phases[\"post_correlation\"]\n\n\nclass SavedQueries:\n    def __init__(self, api, base_api_endpoint, helper=None):\n        self._api = api\n        self._api_endpoint = base_api_endpoint\n        self._queries = {}\n        self._helper = helper\n\n    def get_attributes_by_name(self, query_name):\n\n        if not bool(self._queries):\n            if \"software\" in self._api_endpoint:\n                status, response, exception = self._api.get(\n                    \"/api/queries/saved?filter=module%20in%20%5B%22software%22%5D\",\n                )\n            else:\n                status, response, exception = self._api.get(f\"{self._api_endpoint}/views/saved\")\n\n            if exception is not None:\n                raise Exception(exception)\n\n            for query in response[\"data\"]:\n                self._queries[query[\"attributes\"][\"name\"]] = query[\"attributes\"][\"uuid\"]\n\n        if query_name not in self._queries.keys():\n            raise Exception(f\"Critical error: The saved query '{query_name}' does not exist\")\n        else:\n            uuid = self._queries[query_name]\n\n        for query in response[\"data\"]:\n            if query[\"attributes\"][\"uuid\"] == uuid:\n                query_filter = query[\"attributes\"][\"view\"][\"query\"].get(\"filter\")\n                query_fields = query[\"attributes\"][\"view\"].get(\"fields\")\n\n                query_column_filters = query[\"attributes\"][\"view\"].get(\"colFilters\")\n                query_column_excluded_adapters = query[\"attributes\"][\"view\"].get(\"colExcludedAdapters\")\n                query_asset_exclude_adapters = query[\"attributes\"][\"view\"].get(\"assetExcludeAdapters\")\n                query_asset_condition_expressions = query[\"attributes\"][\"view\"].get(\"assetConditionExpressions\")\n\n        if self._helper:\n            self._helper.log_debug(f\"Found Saved Query {query_name}\")\n            self._helper.log_debug(\n                f\"Saved Query Attributes: uuid: {uuid}, query_filter: {query_filter}, query_fields: {query_fields}, query_column_filters: {query_column_filters}, query_column_excluded_adapters: {query_column_excluded_adapters}, query_asset_exclude_adapters: {query_asset_exclude_adapters}, query_asset_condition_expressions: {query_asset_condition_expressions}\")\n\n        return uuid, query_filter, query_fields, query_column_filters, query_column_excluded_adapters, query_asset_exclude_adapters, query_asset_condition_expressions\n\n\ndef shorten_field_name(field: str) -> str:\n    return field.replace(\"specific_data.data.\", \"\").replace(\"adapters_data.\", \"\")\n\n\nclass EntitySearch:\n    def __init__(self, api, entity_type, page_size=1000, include_details=True, helper=None):\n\n        self._api = api\n        self._api_endpoint = f\"/api/{entity_type}\"\n        self._page_size = page_size\n        if int(self._page_size) > 2000:\n            self._page_size = 2000\n        self._include_details = include_details\n        self._cursor = None\n        self._uuid = None\n        self._query_filter = None\n        self._query_fields = None\n\n        self._query_column_filters = None\n        self._query_column_excluded_adapters = None\n        self._query_asset_exclude_adapters = None\n        self._query_asset_condition_expressions = None\n\n        self._helper = helper\n\n\n    def connection_test(self) -> None:\n        data = {\n            \"data\": {\n                \"type\": \"entity_request_schema\",\n                \"attributes\": {\n                    \"page\": {\n                        \"limit\": 1\n                    },\n                    \"use_cache_entry\": False,\n                    \"always_cached_query\": False,\n                    \"get_metadata\": True,\n                    \"include_details\": True\n                }\n            }\n        }\n\n        status, response, exception = self._api.post(self._api_endpoint, data)\n        if not (status == 200 and response is not None and exception is None):\n            raise Exception(f\"Critical Error! Status Code: {status}\\tException: {exception}\")\n\n\n    def execute_saved_query(self, name, standoff=0, shorten_field_names=False, dynamic_field_mapping={},\n                            incremental_ingest=False, incremental_ingest_time_field='specific_data.data.fetch_time', \n                            include_auids=False, truncate_fields=[], batch_callback=None):\n        try:\n            ax_saved_queries = SavedQueries(self._api, self._api_endpoint, helper=self._helper)\n\n            if self._uuid is None or self._query_filter is None or self._query_fields is None:\n                (self._uuid, \n                 self._query_filter, \n                 self._query_fields, \n                 self._query_column_filters, \n                 self._query_column_excluded_adapters, \n                 self._query_asset_exclude_adapters, \n                 self._query_asset_condition_expressions) = ax_saved_queries.get_attributes_by_name(name)\n\n            if incremental_ingest:\n                if incremental_ingest_time_field not in self._query_fields:\n                    self._query_fields.append(incremental_ingest_time_field)\n\n            if include_auids:\n                if \"internal_axon_id\" not in self._query_fields:\n                    self._query_fields.append(\"internal_axon_id\")\n\n            response = {\"data\": \"init\"}\n            entities = []\n            entity_count = 0\n\n            while response[\"data\"]:\n                data = {\n                    \"data\": {\n                        \"type\": \"entity_request_schema\",\n                        \"attributes\": {\n                            \"use_cache_entry\": False,\n                            \"always_cached_query\": False,\n                            \"filter\": self._query_filter,\n                            \"fields\": {\n                                \"devices\": self._query_fields\n                            },\n                            \"page\": {\n                                \"limit\": self._page_size\n                            },\n                            \"get_metadata\": True,\n                            \"include_details\": self._include_details,\n                            \"use_cursor\": True,\n                            \"cursor_id\": self._cursor\n                        }\n                    }\n                }\n\n                if self._query_column_filters:\n                    data[\"data\"][\"attributes\"][\"field_filters\"] = self._query_column_filters\n                if self._query_column_excluded_adapters:\n                    data[\"data\"][\"attributes\"][\"excluded_adapters\"] = self._query_column_excluded_adapters\n                if self._query_asset_exclude_adapters:\n                    data[\"data\"][\"attributes\"][\"asset_excluded_adapters\"] = self._query_asset_exclude_adapters\n                if self._query_asset_condition_expressions:\n                    data[\"data\"][\"attributes\"][\"asset_filters\"] = self._query_asset_condition_expressions\n\n\n                status, response, exception = self._api.post(self._api_endpoint, data=data)\n\n                self._helper.log_debug(f\"Response: Status: {status}, Data: {response}\")\n\n                if status == 200 and response is not None and exception is None:\n                    if \"meta\" in response:\n                        self._cursor = response[\"meta\"][\"cursor\"]\n                        self._total_assets = response[\"meta\"][\"page\"][\"totalResources\"]\n\n                        for device in response[\"data\"]:\n                            entity_row = {}\n\n                            for field in list(device['attributes'].keys()):\n                                field_name = field\n\n                                if shorten_field_names:\n                                    field_name = shorten_field_name(field)\n\n                                if field_name in dynamic_field_mapping.keys():\n                                    field_name = dynamic_field_mapping[field_name]\n\n                                entity_row[field_name] = device['attributes'][field]\n\n                            entities.append(entity_row)\n\n                        if self._total_assets == len(entities):\n                            response = {\"data\": None}\n\n                    else:\n                        response = {\"data\": None}\n\n                else:\n                    raise Exception(f\"Critical Error! Status Code: '{status}' Exception: '{exception}'\")\n\n                if standoff > 0:\n                    time.sleep(standoff)\n\n                if batch_callback is not None:\n                    if len(entities) > 0:\n                        batch_callback(entities)\n                        entity_count += len(entities)\n                        entities = []\n\n        except Exception as ex:\n            raise Exception(f\"Critical Error! Status Code: Exception: {ex}\")\n\n\nclass EventWriter:\n    def __init__(self, incremental_data_ingest=False, remove_fetch_time_field=False, fetch_time_field_name=None,\n                 checkpoint=None, host=None, source=None, index=None, sourcetype=None, helper=None, event_writer=None):\n        self._incremental_data_ingest = incremental_data_ingest\n        self._remove_fetch_time_field = remove_fetch_time_field\n        self._fetch_time_field_name = fetch_time_field_name\n        self._checkpoint = checkpoint\n        self._host = host\n        self._source = source\n        self._index = index\n        self._sourcetype = sourcetype\n        self._helper = helper\n        self._event_writer = event_writer\n        self._checkpoint = checkpoint\n        self._entity_count = 0\n        self._entity_ids = []\n        self._page = 0\n        self._events_written = 0\n\n    def process_batch(self, entities):\n        # Update entity count\n        self._entity_count += len(entities)\n\n        # Increment page number\n        self._page += 1\n\n        # Log page number and size\n        self._helper.log_info(f\"\"\"Input '{self._helper.get_arg('name')}' - STATS - Processing page {self._page}, \n                            size {len(entities)}\"\"\")\n\n        # Process each entity\n        for entity in entities:\n            if self._helper.get_arg('name') is None:\n                self._entity_ids.append(entity[\"internal_axon_id\"])\n\n            if self._incremental_data_ingest:\n                # Create a timestamp from the devices fetch_time field\n                entity_fetch_time = datetime.datetime.strptime(entity[self._fetch_time_field_name],\n                                                               \"%a, %d %b %Y %H:%M:%S %Z\").timestamp()\n\n                # Remove the fetch_time field if it was not part of the saved query's query_field definition\n                if self._remove_fetch_time_field:\n                    entity.pop(self._fetch_time_field_name)\n\n                # Create event\n                event = self._helper.new_event(source=self._source, host=self._host, index=self._index,\n                                               sourcetype=self._sourcetype, data=json.dumps(entity))\n\n                # Add event if no checkpoint is defined yet, or if fetch time is greater than the checkpoint time\n                if self._checkpoint is None:\n                    self._event_writer.write_event(event)\n                    self._events_written += 1\n                elif entity_fetch_time > self._checkpoint:\n                    self._event_writer.write_event(event)\n                    self._events_written += 1\n            else:\n                # Create event\n                event = self._helper.new_event(source=self._source, host=self._host, index=self._index,\n                                               sourcetype=self._sourcetype, data=json.dumps(entity))\n\n                # Write event\n                self._event_writer.write_event(event)\n                self._events_written += 1\n\n    def get_entity_count(self):\n        return self._entity_count\n\n    def get_events_written(self):\n        return self._events_written\n\n    def get_internal_axon_id_unique_count(self):\n        return len(set(self._entity_ids))\n\n\ndef validate_input(helper, definition):\n    # get Axonius configuration\n    api_host = definition.parameters.get('api_host', str)\n    api_key = definition.parameters.get('api_key', \"\")\n    api_secret = definition.parameters.get('api_secret', \"\")\n\n    # get selected saved query info\n    entity_type = definition.parameters.get('entity_type', str)\n    saved_query = definition.parameters.get('saved_query', str)\n\n    # get extra options\n    page_size = definition.parameters.get('page_size', str)\n    api_standoff = definition.parameters.get('standoff_ms', str)\n    ssl_certificate_path = definition.parameters.get('ssl_certificate_path', \"\")\n\n    if int(page_size) < 1:\n        raise ValueError('\"Page Size\" must be an integer greater than 0')\n\n    if int(api_standoff) < 0:\n        raise ValueError(\n            '\"API Standoff\" must be an integer greater or equal to 0')\n\n    url_parts = urlparse(api_host)\n    if not all([getattr(url_parts, attrs) for attrs in ('scheme', 'netloc')]):\n        raise ValueError('\"The provided URL is invalid.\"')\n\n    if not api_host.startswith('https://'):\n        raise ValueError('\"URL\" must start with https://')\n\n    # Create api object\n    try:\n        verify = True\n\n        if ssl_certificate_path is not None:\n            if len(ssl_certificate_path) > 0:\n                verify = ssl_certificate_path\n\n        helper.log_info(f\"verify: {verify}\")\n\n        api = API(api_host, str(api_key), str(api_secret), verify, helper=helper)\n        search = EntitySearch(api, \"devices\", 1, helper=helper)\n        search.connection_test()\n\n    except Exception as ex:\n        helper.log_info(ex)\n\n        if \"Could not find a suitable TLS CA certificate bundle\" in str(ex):\n            raise ValueError(\"Critical Error, check CA Bundle Path exists and the splunk user has proper permissions\")\n        elif \"SSLCertVerificationError\" in str(ex) or \"Could not find a suitable TLS CA certificate bundle\" in str(ex):\n            raise ValueError(\n                \"The Axonius host fails SSL verification, please review your SSL certificate validation settings\")\n        elif \"Status Code: 401\" not in str(ex):\n            raise ValueError(f\"Critical Error: {ex}\")\n\n    pass\n\n\ndef collect_events(helper, ew):\n    # Retrieve checkpoint\n    checkpoint_name = f\"checkpoint_{helper.get_arg('name')}_{helper.get_arg('entity_type')}_{helper.get_arg('saved_query')}\"\n\n    # get Axonius configuration\n    opt_api_host = helper.get_arg('api_host')\n\n    opt_api_key = helper.get_global_setting('api_key')\n    opt_api_secret = helper.get_global_setting('api_secret')\n\n    # get selected saved query info\n    opt_entity_type = helper.get_arg('entity_type')\n    opt_saved_query = helper.get_arg('saved_query')\n    opt_cron_schedule = helper.get_arg('cron_schedule')\n\n    # get extra options\n    opt_page_size = helper.get_arg('page_size')\n    opt_shorten_field_names = helper.get_arg('shorten_field_names')\n    opt_incremental_data_ingest = helper.get_arg('incremental_data_ingest')\n\n    # create a short version upfront for later just incase\n    opt_incremental_ingest_time_field = helper.get_arg('incremental_ingest_time_field')\n    opt_incremental_ingest_time_field_short = shorten_field_name(opt_incremental_ingest_time_field)\n\n    opt_standoff_ms = helper.get_arg('standoff_ms')\n    opt_field_mapping = helper.get_arg('dynamic_field_mapping')\n    opt_ssl_certificate_path = helper.get_arg('ssl_certificate_path')\n    \n    opt_enable_include_details = helper.get_arg('enable_include_details')\n\n    # extra options to control flow\n    opt_skip_lifecycle_check = helper.get_arg('skip_lifecycle_check')\n\n    # Logging functions\n    def log_info(msg):\n        helper.log_info(f\"Input '{helper.get_arg('name')}' - {msg}\")\n\n    def log_warning(msg):\n        helper.log_warning(f\"Input '{helper.get_arg('name')}' - {msg}\")\n\n    def log_error(msg):\n        helper.log_error(f\"Input '{helper.get_arg('name')}' - {msg}\")\n\n    def log_critical(msg):\n        helper.log_critical(f\"Input '{helper.get_arg('name')}' - {msg}\")\n\n    # Log input variables\n    log_info(f\"VARS - Axonius Host: {opt_api_host}\")\n    log_info(f\"VARS - Entity Type: {opt_entity_type}\")\n    log_info(f\"VARS - Saved Query: {opt_saved_query}\")\n    log_info(f\"VARS - Page Size: {opt_page_size}\")\n    log_info(f\"VARS - Shorten Field Names: {opt_shorten_field_names}\")\n    log_info(f\"VARS - Incremental Ingest: {opt_incremental_data_ingest}\")\n    log_info(f\"VARS - Incremental Ingest Time Field: {opt_incremental_ingest_time_field}\")\n    log_info(f\"VARS - API Standoff (MS): {opt_standoff_ms}\")\n    log_info(f\"VARS - Field Mapping: {opt_field_mapping}\")\n    log_info(f\"VARS - Enable Include Details: {opt_enable_include_details}\")\n    log_info(f\"VARS - CA Bundle Path: {opt_ssl_certificate_path}\")\n    log_info(f\"VARS - Skip Lifecycle Check: {opt_skip_lifecycle_check}\")\n    log_info(f\"VARS - Cron Schedule: {opt_cron_schedule}\")\n\n    include_auids = True if helper.get_arg('name') is None else False\n    critical_error = False\n\n    # Set verify to True/False\n    verify = True\n\n    # Change the value of verify to the path of the ca_bundle if specified\n    if opt_ssl_certificate_path:\n        if len(opt_ssl_certificate_path) > 0:\n            verify = opt_ssl_certificate_path\n\n    # The host field will be used to set the source host in search\n\n    # Pull out just the host information from the Host\n    host = urlparse(opt_api_host).hostname\n\n    if helper.get_global_setting('api_secret'):\n        timeout = int(helper.get_global_setting('https_request_timeout'))\n    else:\n        timeout = Config.request_timeout if helper.get_arg('name') is not None else 5\n\n    retry_standoff = Config.retry_standoff if helper.get_arg('name') is not None else [0, 3, 3, 3]\n\n    # Create an API object for REST calls\n    api = API(opt_api_host, opt_api_key, opt_api_secret, verify, timeout=timeout, helper=helper)\n\n    # Create EntitySearch object with entity type and page size\n    search = EntitySearch(api, opt_entity_type, opt_page_size, opt_enable_include_details, helper=helper)\n\n    log_info(checkpoint_name)\n\n    # Load the input's checkpoint data\n    checkpoint = helper.get_check_point(checkpoint_name)\n\n    time_format = '%Y-%m-%d %H:%M:%S'\n    if checkpoint is not None:\n        log_info(f\"VARS - Check point: {checkpoint_name}\")\n        try:\n            readable_time = datetime.datetime.fromtimestamp(checkpoint, datetime.timezone.utc).strftime(time_format)\n            log_info(f\"VARS - Check point data: {readable_time}\")\n        except:\n            log_info(f\"VARS - Check point data: {checkpoint}\")\n\n    # Default dynamic field names to an empty dict in case opt_field_mapping is empty\n    dynamic_field_names = {}\n\n    # Use dynamic mapping if specified\n    if opt_field_mapping is not None:\n        if len(opt_field_mapping) > 0:\n            try:\n                dynamic_field_names = json.loads(opt_field_mapping)\n            except Exception as ex:\n                pass\n    # Check Cron Schedule if it's time to run\n    to_run_fetch_by_cron = True\n    if opt_cron_schedule and checkpoint:\n        try:\n            # Implicitly convert to datetime so that Croniter won't convert it without utc aware context\n            base_dt = datetime.datetime.fromtimestamp(checkpoint, datetime.timezone.utc)\n            cron = croniter(opt_cron_schedule, base_dt)\n            now__utc_dt = datetime.datetime.now(datetime.timezone.utc)\n            log_info(f\"Current time for Cron check: {now__utc_dt.strftime(time_format)}\")\n            next_schedule_time = cron.get_next(ret_type=float, start_time=base_dt)\n            next_run_dt = datetime.datetime.fromtimestamp(next_schedule_time, datetime.timezone.utc)\n            log_info(f\"Next run is at: {next_run_dt}\")\n            log_info(f'Comparing {now__utc_dt.strftime(time_format)} to {next_run_dt.strftime(time_format)}')\n            if now__utc_dt < next_run_dt:\n                log_info(f\"Data Input not running because it is not time yet.\")\n                to_run_fetch_by_cron = False\n        except Exception as e:\n            log_warning(f\"Exception evaluating cron schedule {opt_cron_schedule} with error {e}\")\n\n    # Retry variables\n    fetch_complete = False\n    exception_thrown = False\n    max_retries = len(retry_standoff)\n    entity_count = 0\n    retries = 0\n    version = None\n    event_writer = None\n    lifecycle_complete = opt_skip_lifecycle_check\n\n    # Set the fetch_time field name, take into account the use of shorten field name\n    fetch_time_field_name = opt_incremental_ingest_time_field_short if opt_shorten_field_names else opt_incremental_ingest_time_field\n\n    while retries < max_retries and not critical_error and not fetch_complete and to_run_fetch_by_cron:\n        try:\n            if version is None:\n                # Get the raw Axonius version from the metadata endpoint\n                metadata = Metadata(api)\n                version = metadata.get_version()\n\n                # Pull out just the host information from the Host\n                match = re.match(\"(\\d+\\_\\d+\\_\\d+)(?:_RC\\d+)\", version)\n\n                # Only set host if the regex exists, match should never be None.\n                if match is not None:\n                    version = match.groups()[0].replace(\"_\", \".\")\n\n                log_info(f\"STATS - Version: {version}\")\n\n                # Turn versions into tuples for equality comparison\n                tup_version = tuple(map(int, (version.split(\".\"))))\n                tup_supported_version = tuple(map(int, (Config.supported_minimum_version.split(\".\"))))\n\n                # If the current version is less than supported, throw a critical exception\n                if tup_version < tup_supported_version:\n                    raise Exception(\"UnsupportedVersion\")\n\n                # Reset retries and exception_thrown\n                retries = 0\n                exception_thrown = False\n\n            if not lifecycle_complete:\n                # Check if a discovery is running and correlation hasn't complete, warn customer if true\n                lifecycle = Lifecycle(api)\n\n                if lifecycle.discovery_is_running() and not lifecycle.correlation_is_complete():\n                    log_warning(f\"Warning: Fetch started while correlation was not complete.\")\n\n                lifecycle_complete = True\n\n                # Reset retries and exception_thrown\n                retries = 0\n                exception_thrown = False\n\n            if not event_writer:\n                # Get definition of query_fields, used to check if the fetch_time field should be removed\n                api_endpoint = f\"/api/{opt_entity_type}\"\n                ax_saved_queries = SavedQueries(api, api_endpoint, helper=helper)\n                (uuid, \n                 query_filter, \n                 query_fields, \n                 query_column_filters, \n                 query_column_excluded_adapters, \n                 query_asset_exclude_adapters, \n                 query_asset_condition_expressions) = ax_saved_queries.get_attributes_by_name(opt_saved_query)\n\n                # Default remove fetch time to true\n                remove_fetch_time_field = True\n\n                # Look for fetch_time in the query_fields definition of the specified saved query\n                if opt_shorten_field_names:\n                    if fetch_time_field_name in query_fields:\n                        remove_fetch_time_field = False\n\n                # Create EventWriter instance to process batches\n                event_writer = EventWriter(incremental_data_ingest=opt_incremental_data_ingest,\n                                           remove_fetch_time_field=remove_fetch_time_field,\n                                           fetch_time_field_name=fetch_time_field_name, checkpoint=checkpoint,\n                                           host=host, source=helper.get_arg('name'), index=helper.get_output_index(),\n                                           sourcetype=helper.get_sourcetype(), helper=helper, event_writer=ew)\n\n                # Reset retries and exception_thrown\n                retries = 0\n                exception_thrown = False\n\n            # Grab entity from the saved search\n            search.execute_saved_query(opt_saved_query, int(opt_standoff_ms) / 1000, opt_shorten_field_names,\n                                       dynamic_field_names, incremental_ingest=opt_incremental_data_ingest, \n                                       incremental_ingest_time_field=opt_incremental_ingest_time_field,\n                                       include_auids=include_auids, batch_callback=event_writer.process_batch)\n\n            # Get Stats\n            entity_count = event_writer.get_entity_count()\n            events_written = event_writer.get_events_written()\n\n            # Fetch is complete, see below for consistency checks if an exception was thrown during fetch\n            fetch_complete = True\n\n            # Log stats\n            log_info(f\"STATS - Total entities returned: {entity_count}\")\n            log_info(f\"STATS - Total events written: {events_written}\")\n\n            # Sanity check for unique ids, the number needs to match entity_count\n            if helper.get_arg('name') is None:\n                log_info(f\"STATS - Total unique ids: {event_writer.get_internal_axon_id_unique_count()}\")\n        except Exception as ex:\n            # Die if running an unsupported version of Axonius, or log the error and track for retry purposes\n            if \"UnsupportedVersion\" in str(ex):\n                critical_error = True\n            else:\n                log_error(f\"ERR - Error '{ex}'\")\n                exception_thrown = True\n\n        if critical_error:\n            log_critical(\n                f\"Critical Error: Axonius version {version} is unsupported, the minimum version is {Config.supported_minimum_version}\")\n        elif exception_thrown and not fetch_complete:\n            # Increment retry counter\n            retries += 1\n\n            if retries < max_retries:\n                # Log retry number and display the standoff\n                log_info(f\"COLL - Retry {retries} sleeping for {retry_standoff[retries]} seconds, then retrying\")\n\n                # Sleep the process and then retry\n                time.sleep(retry_standoff[retries])\n            else:\n                # Log no devices after max retries\n                log_critical(f\"Critical Error: Unable to complete fetch due to unrecoverable errors.\")\n        elif exception_thrown and fetch_complete:\n            # Log recovered from error during fetch\n            log_warning(f\"Warning: Fetch was interrupted by a transient error, review results for fetch completeness.\")\n        else:\n            # Save new checkpoint if entity_count is greater than one\n            if entity_count > 0:\n                current_utc_ts = datetime.datetime.now(datetime.timezone.utc).timestamp()\n                log_info(f'Saving checkpoint {current_utc_ts}')\n                helper.save_check_point(checkpoint_name, current_utc_ts)\n", "customized_options": [{"name": "api_host", "value": ""}, {"name": "entity_type", "value": "users"}, {"name": "saved_query", "value": ""}, {"name": "page_size", "value": "1000"}, {"name": "standoff_ms", "value": "0"}, {"name": "shorten_field_names", "value": false}, {"name": "dynamic_field_mapping", "value": "{\"hostname\": \"host\", \"network_interfaces.ips\": \"ip_address\"}"}, {"name": "cron_schedule", "value": ""}, {"name": "incremental_data_ingest", "value": true}, {"name": "incremental_ingest_time_field", "value": "specific_data.data.fetch_time"}, {"name": "enable_include_details", "value": true}, {"name": "ssl_certificate_path", "value": ""}, {"name": "skip_lifecycle_check", "value": false}], "uuid": "55f2fe0466d54fcfabd0fbc3a35d28c1", "sample_count": "16"}]}, "field_extraction_builder": {"axonius_saved_query": {"is_parsed": true, "data_format": "json"}}, "global_settings_builder": {"global_settings": {"log_settings": {"log_level": "DEBUG"}, "customized_settings": [{"required": false, "name": "api_key", "label": "API Key", "placeholder": "", "default_value": "", "help_string": "The API Key from https://axonius.example.com/account/api-key", "type": "password", "format_type": "password", "value": ""}, {"required": false, "name": "api_secret", "label": "API Secret", "placeholder": "", "default_value": "", "help_string": "The API Secret from https://axonius.example.com/account/api-key", "type": "password", "format_type": "password", "value": ""}, {"required": false, "name": "https_request_timeout", "label": "https Request Timeout", "default_value": "900", "placeholder": "", "help_string": "How many seconds before a request timesout to the Axonius Host", "type": "text", "format_type": "text", "value": "900"}]}}, "sourcetype_builder": {"axonius_saved_query": {"metadata": {"event_count": 0, "data_input_name": "axonius_saved_query", "extractions_count": 0, "cims_count": 0}}}, "validation": {"validators": ["best_practice_validation", "data_model_mapping_validation", "field_extract_validation", "app_cert_validation"], "status": "job_started", "validation_id": "v_1739965756_73"}}